{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ‰‹æŠŠæ‰‹æ•™ä½ å†™ SelfAttV4 (Step-by-Step)\n",
    "\n",
    "ä½ å¥½ï¼æœ¬æ•™ç¨‹å°†å¸¦ä½ æŠŠæä¾›çš„ `SelfAttV4` ä»£ç æ‹†è§£å¼€æ¥ï¼Œä¸€æ­¥æ­¥å®žçŽ°ã€‚\n",
    "\n",
    "**ðŸ’¡ äº’åŠ¨æ–¹å¼**ï¼š\n",
    "å¦‚æžœä½ åœ¨é˜…è¯»ä»£ç æˆ–è¿è¡Œè¿‡ç¨‹ä¸­æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·ç›´æŽ¥åœ¨ä»£ç å•å…ƒæ ¼ä¸­æ·»åŠ æ³¨é‡Šæé—®ï¼Œä¾‹å¦‚ï¼š\n",
    "```python\n",
    "# ? ä¸ºä»€ä¹ˆè¿™é‡Œè¦é™¤ä»¥ sqrt(dim)ï¼Ÿ\n",
    "# Question: è¿™ä¸ª dropout æ˜¯å¿…é¡»çš„å—ï¼Ÿ\n",
    "```\n",
    "æˆ‘ä¼šè¯»å–ä½ çš„æ³¨é‡Šå¹¶ç›´æŽ¥åœ¨ notebook ä¸­å›žå¤ä½ ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥ä¸Žå‡†å¤‡\n",
    "\n",
    "é¦–å…ˆå¯¼å…¥å¿…è¦çš„åº“ã€‚`math` ç”¨äºŽè®¡ç®—å¹³æ–¹æ ¹ï¼Œ`torch` æ˜¯æ ¸å¿ƒæ¡†æž¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ä»¥ä¾¿å¤çŽ°ç»“æžœ\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"çŽ¯å¢ƒå‡†å¤‡å°±ç»ªï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äºŒæ­¥ï¼šå®šä¹‰æ¨¡åž‹ç»“æž„ (__init__)\n",
    "\n",
    "åœ¨åˆå§‹åŒ–å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰æ‰€æœ‰çš„å±‚ã€‚ä½ çš„ä»£ç ä¸­ä½¿ç”¨äº†ä¸‰ä¸ªç‹¬ç«‹çš„çº¿æ€§å±‚åˆ†åˆ«æ˜ å°„ Q, K, Vï¼Œè¿™ç§å†™æ³•éžå¸¸æ¸…æ™°ã€‚\n",
    "\n",
    "æˆ‘ä»¬è¿˜å®šä¹‰äº† `output_proj` å’Œ `att_drop`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttV4_Init(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim   \n",
    "\n",
    "        # 1. å®šä¹‰ Q, K, V çš„æŠ•å½±å±‚\n",
    "        # è¿™æ ·å¾ˆæ¸…æ™°ï¼Œæ¯ä¸ªéƒ½æœ‰ç‹¬ç«‹çš„å‚æ•°\n",
    "        self.query_proj = nn.Linear(dim, dim)\n",
    "        self.key_proj = nn.Linear(dim, dim)\n",
    "        self.value_proj = nn.Linear(dim, dim)\n",
    "\n",
    "        # 2. å®šä¹‰ Dropout\n",
    "        # ä¸€èˆ¬æ˜¯ 0.1 çš„ dropout\n",
    "        self.att_drop = nn.Dropout(0.1)\n",
    "\n",
    "        # 3. å®šä¹‰è¾“å‡ºæŠ•å½±\n",
    "        # è¿™æ˜¯ MultiHeadAttention ä¸­çš„äº§ç‰©ï¼Œå³ä½¿æ˜¯å•å¤´ä¹Ÿå¯ä»¥ä¿ç•™\n",
    "        self.output_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # æš‚æ—¶ç•™ç©ºï¼Œä¸‹ä¸€æ­¥å®žçŽ°\n",
    "        pass\n",
    "\n",
    "# æµ‹è¯•åˆå§‹åŒ–\n",
    "model_init = SelfAttV4_Init(dim=2)\n",
    "print(model_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸‰æ­¥ï¼šè®¡ç®— Attention Scores\n",
    "\n",
    "è¿™æ˜¯æœ€æ ¸å¿ƒçš„æ­¥éª¤ï¼š\n",
    "1. é€šè¿‡æŠ•å½±å±‚å¾—åˆ° Q, K, Vã€‚\n",
    "2. è®¡ç®— $QK^T$ã€‚\n",
    "3. é™¤ä»¥ $\\sqrt{d}$ è¿›è¡Œç¼©æ”¾ (Scale)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(self, X):\n",
    "    # X shape: (batch, seq, dim)\n",
    "    \n",
    "    # 1. æŠ•å½±\n",
    "    Q = self.query_proj(X)\n",
    "    K = self.key_proj(X)\n",
    "    V = self.value_proj(X)\n",
    "    \n",
    "    # 2. çŸ©é˜µä¹˜æ³• Q @ K^T\n",
    "    # K.transpose(-1, -2) å°†æœ€åŽä¸¤ä¸ªç»´åº¦äº¤æ¢ï¼Œ(batch, seq, dim) -> (batch, dim, seq)\n",
    "    # ç»“æžœ shape: (batch, seq, seq)\n",
    "    att_weight = Q @ K.transpose(-1, -2) \n",
    "    \n",
    "    # 3. ç¼©æ”¾\n",
    "    att_weight = att_weight / math.sqrt(self.dim)\n",
    "    \n",
    "    return att_weight, V\n",
    "\n",
    "# è¿™é‡Œçš„ä»£ç ä»…ä½œæ¼”ç¤ºé€»è¾‘ï¼Œä¸èƒ½ç›´æŽ¥è¿è¡Œï¼Œå› ä¸ºå®ƒä¾èµ– self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬å››æ­¥ï¼šMasking (æŽ©ç )\n",
    "\n",
    "å¦‚æžœæœ‰ `attention_mask`ï¼Œæˆ‘ä»¬éœ€è¦æŠŠ mask ä¸º 0 çš„ä½ç½®æ›¿æ¢æˆä¸€ä¸ªæžå°çš„æ•°ï¼ˆä¾‹å¦‚ `-1e20`ï¼‰ï¼Œè¿™æ ·åœ¨ Softmax åŽå®ƒä»¬ä¼šå˜æˆ 0ï¼Œè¡¨ç¤ºä¸å…³æ³¨è¿™äº›ä½ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(att_weight, attention_mask):\n",
    "    if attention_mask is not None:\n",
    "        # ç»™ weight å¡«å……ä¸€ä¸ªæžå°çš„å€¼\n",
    "        # attention_mask == 0 çš„ä½ç½®ä¼šè¢«å¡«å……\n",
    "        att_weight = att_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "    return att_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬äº”æ­¥ï¼šSoftmax, Dropout ä¸Ž Output\n",
    "\n",
    "1. **Softmax**: å°†åˆ†æ•°å½’ä¸€åŒ–ä¸ºæ¦‚çŽ‡ã€‚\n",
    "2. **Dropout**: è¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒç‰¹æ®Šçš„æ“ä½œï¼Œåœ¨ BERT å®˜æ–¹ä»£ç ä¸­å‡ºçŽ°è¿‡ï¼Œè™½ç„¶æœ‰äº›å¥‡æ€ªï¼Œä½†ä¿ç•™å®ƒæ˜¯ä¸ºäº†å¯¹é½ã€‚\n",
    "3. **Weighted Sum**: `Attention * V`ã€‚\n",
    "4. **Output Project**: æœ€åŽçš„çº¿æ€§å±‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize(self, att_weight, V):\n",
    "    # 1. Softmax\n",
    "    att_weight = torch.softmax(att_weight, dim=-1)\n",
    "    \n",
    "    # æ‰“å°ä¸€ä¸‹çœ‹çœ‹æƒé‡åˆ†å¸ƒ\n",
    "    # print(att_weight) \n",
    "\n",
    "    # 2. Dropout (ç‰¹æ®Šä½ç½®)\n",
    "    # è¿™é‡Œåœ¨ BERTä¸­çš„å®˜æ–¹ä»£ç ä¹Ÿè¯´å¾ˆå¥‡æ€ªï¼Œä½†æ˜¯åŽŸæ–‡ä¸­è¿™ä¹ˆç”¨äº†ï¼Œæ‰€ä»¥ç»§æ‰¿äº†ä¸‹æ¥\n",
    "    att_weight = self.att_drop(att_weight)\n",
    "\n",
    "    # 3. åŠ æƒæ±‚å’Œ\n",
    "    output = att_weight @ V\n",
    "    \n",
    "    # 4. è¾“å‡ºæŠ•å½±\n",
    "    ret = self.output_proj(output)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®Œæ•´ä»£ç ï¼šSelfAttV4\n",
    "\n",
    "çŽ°åœ¨æˆ‘ä»¬å°†æ‰€æœ‰æ­¥éª¤æ•´åˆåˆ°ä¸€èµ·ï¼Œè¿™æ­£æ˜¯ä½ æä¾›çš„å®Œæ•´ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttV4(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        # è¿™æ ·å¾ˆæ¸…æ™°\n",
    "        self.query_proj = nn.Linear(dim, dim)\n",
    "        self.key_proj = nn.Linear(dim, dim)\n",
    "        self.value_proj = nn.Linear(dim, dim)\n",
    "        # ä¸€èˆ¬æ˜¯ 0.1 çš„ dropoutï¼Œä¸€èˆ¬å†™ä½œ config.attention_probs_dropout_prob\n",
    "        # hidden_dropout_prob ä¸€èˆ¬ä¹Ÿæ˜¯ 0.1\n",
    "        self.att_drop = nn.Dropout(0.1)\n",
    "\n",
    "        # å¯ä»¥ä¸å†™ï¼›å…·ä½“å’Œé¢è¯•å®˜æ²Ÿé€šã€‚\n",
    "        # è¿™æ˜¯ MultiHeadAttention ä¸­çš„äº§ç‰©ï¼Œè¿™ä¸ªç•™ç»™ MultiHeadAttention ä¹Ÿæ²¡æœ‰é—®é¢˜ï¼›\n",
    "        self.output_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # attention_mask shape is: (batch, seq)\n",
    "        # X shape is: (batch, seq, dim)\n",
    "\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "\n",
    "        att_weight = Q @ K.transpose(-1, -2) / math.sqrt(self.dim)\n",
    "        if attention_mask is not None:\n",
    "            # ç»™ weight å¡«å……ä¸€ä¸ªæžå°çš„å€¼\n",
    "            att_weight = att_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        att_weight = torch.softmax(att_weight, dim=-1)\n",
    "        print(\"Attention Weights:\", att_weight)\n",
    "\n",
    "        # è¿™é‡Œåœ¨ BERTä¸­çš„å®˜æ–¹ä»£ç ä¹Ÿè¯´å¾ˆå¥‡æ€ªï¼Œä½†æ˜¯åŽŸæ–‡ä¸­è¿™ä¹ˆç”¨äº†ï¼Œæ‰€ä»¥ç»§æ‰¿äº†ä¸‹æ¥\n",
    "        # ï¼ˆç”¨äºŽ output åŽé¢ä¼šæ›´ç¬¦åˆç›´è§‰ï¼Ÿï¼‰\n",
    "        att_weight = self.att_drop(att_weight)\n",
    "\n",
    "        output = att_weight @ V\n",
    "        ret = self.output_proj(output)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¿è¡Œæµ‹è¯•\n",
    "\n",
    "æœ€åŽï¼Œæˆ‘ä»¬ä½¿ç”¨ä½ æä¾›çš„æµ‹è¯•æ•°æ®æ¥è¿è¡Œæ¨¡åž‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(3, 4, 2)\n",
    "b = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "    ]\n",
    " )\n",
    "print(\"b shape:\", b.shape)\n",
    "\n",
    "# æž„é€  mask\n",
    "# b.unsqueeze(dim=1) -> (3, 1, 4)\n",
    "# .repeat(1, 4, 1) -> (3, 4, 4)\n",
    "mask = b.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "print(\"Mask shape:\", mask.shape)\n",
    "\n",
    "net = SelfAttV4(2)\n",
    "output = net(X, mask)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
