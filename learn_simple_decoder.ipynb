{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手把手教你写 Transformer Decoder Block\n",
    "\n",
    "在这个教程中，我们将基于你提供的代码，一步步解析并实现一个 Transformer Decoder 模块。\n",
    "\n",
    "我们将重点关注以下几个核心组件：\n",
    "1.  **Multi-Head Attention (多头注意力)**：如何拆分头，以及如何计算缩放点积注意力。\n",
    "2.  **Causal Masking (因果掩码)**：如何确保模型看不到未来的信息。\n",
    "3.  **Feed-Forward Network (前馈网络)**：两层线性变换加上激活函数。\n",
    "4.  **Residual Connection & LayerNorm (残差连接与层归一化)**：保持梯度流动和训练稳定性。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步：环境准备与导入\n",
    "\n",
    "首先，我们需要导入 PyTorch 和必要的数学库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "print(\"环境准备就绪！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步：核心注意力计算 (Attention Output)\n",
    "\n",
    "这是 Transformer 中最复杂的数学部分。我们需要计算 $ \\text{softmax}(\\\\frac{QK^T}{\\\\sqrt{d}})V $。\n",
    "\n",
    "在 Decoder 中，我们还需要加上 **Attention Mask**，通常是下三角矩阵，防止模型“作弊”看到未来的 token。\n",
    "\n",
    "让我们先把这个核心逻辑提取出来理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_output_logic(query, key, value, head_dim, dropout_layer, attention_mask=None):\n",
    "    # query, key, value 的形状: (batch, nums_head, seq, head_dim)\n",
    "    \n",
    "    # 1. 计算相关性 (Attention Scores)\n",
    "    # 我们需要 Q @ K^T。注意 key 需要转置最后两个维度来匹配矩阵乘法\n",
    "    key = key.transpose(2, 3)  # 变成 (batch, num_head, head_dim, seq)\n",
    "    att_weight = torch.matmul(query, key) / math.sqrt(head_dim)\n",
    "\n",
    "    # 2. 应用 Attention Mask (Causal Masking)\n",
    "    if attention_mask is not None:\n",
    "        # 确保 mask 是下三角矩阵 (只看过去)\n",
    "        attention_mask = attention_mask.tril()\n",
    "        # 将 mask 为 0 的位置填充为极小的负数 (softmax 后变为 0)\n",
    "        att_weight = att_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "    else:\n",
    "        # 如果没有提供 mask，我们人工构造一个默认的下三角 mask\n",
    "        attention_mask = torch.ones_like(att_weight).tril()\n",
    "        att_weight = att_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "\n",
    "    # 3. Softmax 归一化\n",
    "    att_weight = torch.softmax(att_weight, dim=-1)\n",
    "    # (可选) 打印权重用于调试\n",
    "    # print(\"Attention Weights sample:\", att_weight[0, 0, 0, :]) \n",
    "\n",
    "    # 4. Dropout (防止过拟合)\n",
    "    att_weight = dropout_layer(att_weight)\n",
    "\n",
    "    # 5. 加权求和 (Weighted Sum)\n",
    "    mid_output = torch.matmul(att_weight, value)\n",
    "    # output shape: (batch, nums_head, seq, head_dim)\n",
    "    \n",
    "    return mid_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：构建 SimpleDecoder 类\n",
    "\n",
    "现在我们将所有的组件封装成一个 `nn.Module` 类。\n",
    "\n",
    "这个类主要包含三个部分：\n",
    "1.  **`__init__`**: 定义所有层（线性层、Norm、Dropout）。\n",
    "2.  **`attention_block`**: 处理 Q/K/V 的投影、分头（Reshape）、以及残差连接。\n",
    "3.  **`ffn_block`**: 前馈神经网络部分。\n",
    "4.  **`forward`**: 串联整个流程。\n",
    "\n",
    "### 关于 Normalization (归一化)\n",
    "你提供的代码使用的是 **Post-Norm** (先相加，再 Norm)：\n",
    "`Norm(x + sublayer(x))`\n",
    "\n",
    "*注：现在的 LLaMA 等模型更流行 Pre-Norm (先 Norm，再进入子层)，以及 RMSNorm。但这里我们严格按照你的代码实现 Post-Norm 和 LayerNorm。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, nums_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nums_head = nums_head\n",
    "        self.head_dim = hidden_dim // nums_head\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # --- Layers Definition ---\n",
    "        \n",
    "        # 1. Attention 相关的层\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim) # Output projection\n",
    "        self.drop_att = nn.Dropout(self.dropout)\n",
    "        self.layernorm_att = nn.LayerNorm(hidden_dim, eps=0.00001)\n",
    "\n",
    "        # 2. FFN (Feed-Forward Network) 相关的层\n",
    "        self.up_proj = nn.Linear(hidden_dim, hidden_dim * 4)   # 放大 4 倍\n",
    "        self.down_proj = nn.Linear(hidden_dim * 4, hidden_dim) # 缩回原维度\n",
    "        self.act_fn = nn.ReLU() # 激活函数\n",
    "        self.drop_ffn = nn.Dropout(self.dropout)\n",
    "        self.layernorm_ffn = nn.LayerNorm(hidden_dim, eps=0.00001)\n",
    "\n",
    "    # --- 核心逻辑 1: Attention 计算 ---\n",
    "    def attention_output(self, query, key, value, attention_mask=None):\n",
    "        # 这里就是我们在第二步中实现的逻辑\n",
    "        \n",
    "        # key shape: (batch, num_head, seq, head_dim) -> transpose -> (batch, num_head, head_dim, seq)\n",
    "        key = key.transpose(2, 3) \n",
    "        \n",
    "        # Scaled Dot-Product\n",
    "        att_weight = torch.matmul(query, key) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Causal Masking\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.tril()\n",
    "            att_weight = att_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "        else:\n",
    "            attention_mask = torch.ones_like(att_weight).tril()\n",
    "            att_weight = att_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        att_weight = torch.softmax(att_weight, dim=-1)\n",
    "        # print(\"Attention weights max:\", att_weight.max().item()) # Debug\n",
    "\n",
    "        att_weight = self.drop_att(att_weight)\n",
    "        mid_output = torch.matmul(att_weight, value)\n",
    "        \n",
    "        # mid_output: (batch, nums_head, seq, head_dim)\n",
    "        # 我们需要把它还原回 (batch, seq, hidden_dim)\n",
    "        \n",
    "        # transpose(1, 2) 交换 nums_head 和 seq -> (batch, seq, nums_head, head_dim)\n",
    "        mid_output = mid_output.transpose(1, 2).contiguous()\n",
    "        \n",
    "        batch, seq, _, _ = mid_output.size()\n",
    "        # view 将最后两维合并: nums_head * head_dim = hidden_dim\n",
    "        mid_output = mid_output.view(batch, seq, -1)\n",
    "        \n",
    "        # 最后的线性投影\n",
    "        output = self.o_proj(mid_output)\n",
    "        return output\n",
    "\n",
    "    # --- 模块 1: Attention Block (含 Projection 和 Norm) ---\n",
    "    def attention_block(self, X, attention_mask=None):\n",
    "        batch, seq, _ = X.size()\n",
    "        \n",
    "        # 1. 投影 (Linear Projections)\n",
    "        # 2. 分头 (Split Heads): view -> (batch, seq, head, head_dim)\n",
    "        # 3. 换轴 (Transpose):   transpose -> (batch, head, seq, head_dim)\n",
    "        # 这样做是为了让 attention 计算时，head 维度独立，seq 维度参与矩阵乘法\n",
    "        query = self.q_proj(X).view(batch, seq, self.nums_head, -1).transpose(1, 2)\n",
    "        key = self.k_proj(X).view(batch, seq, self.nums_head, -1).transpose(1, 2)\n",
    "        value = self.v_proj(X).view(batch, seq, self.nums_head, -1).transpose(1, 2)\n",
    "\n",
    "        output = self.attention_output(\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        \n",
    "        # Post-Norm: Norm(X + Output)\n",
    "        return self.layernorm_att(X + output)\n",
    "\n",
    "    # --- 模块 2: Feed Forward Block ---\n",
    "    def ffn_block(self, X):\n",
    "        # 1. Up projection (expand)\n",
    "        up = self.act_fn(self.up_proj(X))\n",
    "        \n",
    "        # 2. Down projection (contract)\n",
    "        down = self.down_proj(up)\n",
    "\n",
    "        # 3. Dropout\n",
    "        down = self.drop_ffn(down)\n",
    "\n",
    "        # 4. Post-Norm: Norm(X + Output)\n",
    "        return self.layernorm_ffn(X + down)\n",
    "\n",
    "    # --- 主流程 Forward ---\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # X: (batch, seq, hidden_dim)\n",
    "        \n",
    "        # 1. 先过 Attention Block\n",
    "        att_output = self.attention_block(X, attention_mask=attention_mask)\n",
    "        \n",
    "        # 2. 再过 FFN Block\n",
    "        ffn_output = self.ffn_block(att_output)\n",
    "        \n",
    "        return ffn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步：测试运行\n",
    "\n",
    "最后，我们使用你提供的测试数据来验证模型的输出形状是否正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造输入数据\n",
    "x = torch.rand(3, 4, 64) # (Batch=3, Seq=4, Hidden=64)\n",
    "net = SimpleDecoder(64, 8) # Hidden=64, Heads=8\n",
    "\n",
    "# 构造 Mask\n",
    "# 原始 Mask: (3, 4)\n",
    "# 目标 Mask: (Batch, Num_Heads, Seq, Seq) 用于 broadcast\n",
    "mask = (\n",
    "    torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 1, 0]])\n",
    "    .unsqueeze(1) # -> (3, 1, 4)\n",
    "    .unsqueeze(2) # -> (3, 1, 1, 4)\n",
    "    .repeat(1, 8, 4, 1) # -> (3, 8, 4, 4)\n",
    ")\n",
    "\n",
    "output = net(x, mask)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# 验证输出形状应与输入一致\n",
    "assert output.shape == x.shape, \"Output shape mismatch!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
