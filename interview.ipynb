{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f591f6a6",
      "metadata": {},
      "source": [
        "# Interview Transformer (Step-by-Step)\n",
        "\n",
        "Goal: hand-write a minimal GPT-style Transformer in PyTorch, step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97aae41a",
      "metadata": {},
      "source": [
        "## Step 0: Imports + seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5ef2d7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "seed = 1337\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66ba3791",
      "metadata": {},
      "source": [
        "## Step 1: Tiny dataset + vocab\n",
        "We use a short text for fast iteration. You can replace it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8610c163",
      "metadata": {},
      "outputs": [],
      "source": [
        "text = '''\n",
        "To be, or not to be, that is the question:\n",
        "Whether 'tis nobler in the mind to suffer\n",
        "The slings and arrows of outrageous fortune,\n",
        "Or to take arms against a sea of troubles\n",
        "'''.strip()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"vocab_size:\", vocab_size)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda ids: \"\".join([itos[i] for i in ids])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(\"data length:\", len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6d88d75",
      "metadata": {},
      "source": [
        "## Step 2: Batch sampling (causal language modeling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ec0898",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/val split\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Batch config\n",
        "block_size = 64\n",
        "batch_size = 16\n",
        "\n",
        "def get_batch(split):\n",
        "    src = train_data if split == \"train\" else val_data\n",
        "    # shrink block if split is small\n",
        "    t = min(block_size, len(src) - 1)\n",
        "    if t < 1:\n",
        "        raise ValueError(\"Text too short for batching; need at least 2 tokens.\")\n",
        "    max_start = len(src) - t\n",
        "    ix = torch.randint(max_start, (batch_size,))\n",
        "    x = torch.stack([src[i:i+t] for i in ix])\n",
        "    y = torch.stack([src[i+1:i+t+1] for i in ix])\n",
        "    return x.to(device), y.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c272d5e",
      "metadata": {},
      "source": [
        "## Step 3: Scaled Dot-Product Attention (single head)\n",
        "We'll write this from scratch next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fc6a7f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: implement a single-head causal self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2ac1c22",
      "metadata": {},
      "source": [
        "## Step 4: Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a11c859",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: wrap multiple heads + output projection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbf2230b",
      "metadata": {},
      "source": [
        "## Step 5: FeedForward + Transformer Block (Pre-LN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5646fb70",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: implement FFN and residual block"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86cc00d9",
      "metadata": {},
      "source": [
        "## Step 6: GPT model (token + position embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49bffdf8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: implement GPT model with generate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23e83a46",
      "metadata": {},
      "source": [
        "## Step 7: Train + Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "417561a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: training loop and text generation"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
