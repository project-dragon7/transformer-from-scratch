{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# æ‰‹æŠŠæ‰‹æ•™ä½ å†™ Transformer (Step-by-Step)\n",
        "\n",
        "æ¬¢è¿ï¼æœ¬æ•™ç¨‹å°†å¸¦ä½ ä»é›¶å¼€å§‹ï¼Œä¸€è¡Œä¸€è¡Œåœ°æ„å»ºä¸€ä¸ª GPT é£æ ¼çš„ Transformer æ¨¡å‹ã€‚\n",
        "\n",
        "æˆ‘ä»¬å°†ä» Transformer çš„çµé­‚ â€”â€” **Scaled Dot-Product Attention (ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›)** å¼€å§‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ç¯å¢ƒå‡†å¤‡å°±ç»ªï¼\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "print(\"ç¯å¢ƒå‡†å¤‡å°±ç»ªï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬ä¸€æ­¥ï¼šç†è§£æ ¸å¿ƒå…¬å¼\n",
        "\n",
        "Attention çš„æ ¸å¿ƒå…¬å¼å¦‚ä¸‹ï¼š\n",
        "\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
        "\n",
        "- **Q (Query)**: æŸ¥è¯¢å‘é‡ (æ¯”å¦‚æˆ‘æ‰‹é‡Œæ‹¿ç€çš„ token)\n",
        "- **K (Key)**: é”®å‘é‡ (æ¯”å¦‚è¢«æŸ¥è¯¢çš„ token)\n",
        "- **V (Value)**: å€¼å‘é‡ (å¦‚æœä¸ç›¸å…³ï¼Œæˆ‘å°±ä¸å…³æ³¨ä½ ï¼›å¦‚æœç›¸å…³ï¼Œæˆ‘ä»ä½ è¿™é‡Œæå–ä»€ä¹ˆä¿¡æ¯)\n",
        "\n",
        "è®©æˆ‘ä»¬å…ˆç”¨éšæœºç”Ÿæˆçš„ Tensor æ¥æ¨¡æ‹Ÿè¿™ä¸ªè¿‡ç¨‹ï¼Œä¸æ¶‰åŠä»»ä½•ç¥ç»ç½‘ç»œå±‚ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q shape: torch.Size([1, 8, 16])\n",
            "K shape: torch.Size([1, 8, 16])\n",
            "V shape: torch.Size([1, 8, 16])\n"
          ]
        }
      ],
      "source": [
        "# 1. å®šä¹‰æ•°æ®çš„ç»´åº¦\n",
        "B = 1  # Batch size (æ‰¹æ¬¡å¤§å°)\n",
        "T = 8  # Time steps (åºåˆ—é•¿åº¦ï¼Œæ¯”å¦‚ \"I love deep learning\" æ˜¯ 4 ä¸ª tokenï¼Œè¿™é‡Œæˆ‘ä»¬è®¾ä¸º 8)\n",
        "C = 32 # Channels (embedding sizeï¼Œæ¯ä¸ª token ç”¨å¤šå°‘ç»´å‘é‡è¡¨ç¤º)\n",
        "head_size = 16 # å•ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ (d_k)\n",
        "\n",
        "# 2. ç”Ÿæˆéšæœºçš„ Q, K, V\n",
        "# åœ¨çœŸå®æ¨¡å‹ä¸­ï¼Œè¿™äº›æ˜¯ç”± input x ç»è¿‡ nn.Linear æŠ•å½±å¾—åˆ°çš„\n",
        "key   = torch.randn(B, T, head_size)\n",
        "query = torch.randn(B, T, head_size)\n",
        "value = torch.randn(B, T, head_size)\n",
        "\n",
        "print(f\"Q shape: {query.shape}\")\n",
        "print(f\"K shape: {key.shape}\")\n",
        "print(f\"V shape: {value.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“ ç»ƒä¹  1: åŸå§‹æ³¨æ„åŠ›è®¡ç®—\n",
        "\n",
        "è¯·å°è¯•è¡¥å…¨ä¸‹é¢çš„ä»£ç ï¼Œå®ç°ä¸Šè¿°å…¬å¼ã€‚\n",
        "\n",
        "**æç¤º**ï¼š\n",
        "1. çŸ©é˜µä¹˜æ³• $QK^T$: ä½¿ç”¨ `@` è¿ç®—ç¬¦æˆ– `torch.matmul`ã€‚æ³¨æ„ `K` éœ€è¦è½¬ç½®æœ€åä¸¤ä¸ªç»´åº¦ã€‚\n",
        "2. è½¬ç½®æ“ä½œ: `k.transpose(-2, -1)`\n",
        "3. ç¼©æ”¾: é™¤ä»¥ `head_size` çš„å¹³æ–¹æ ¹ã€‚\n",
        "4. Softmax: `F.softmax(..., dim=-1)`\n",
        "5. åŠ æƒæ±‚å’Œ: æ³¨æ„åŠ›æƒé‡çŸ©é˜µä¹˜ä»¥ `V`ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'V' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out, weights\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# è¿è¡Œæµ‹è¯•ï¼ˆå–æ¶ˆä¸‹é¢ä¸€è¡Œçš„æ³¨é‡Šæ¥æµ‹è¯•ï¼‰\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m out, weights = \u001b[43mraw_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOutput shape:\u001b[39m\u001b[33m\"\u001b[39m, out.shape) \u001b[38;5;66;03m# åº”è¯¥æ˜¯ (1, 8, 16)\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWeights shape:\u001b[39m\u001b[33m\"\u001b[39m, weights.shape) \u001b[38;5;66;03m# åº”è¯¥æ˜¯ (1, 8, 8)\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mraw_attention\u001b[39m\u001b[34m(q, k, v)\u001b[39m\n\u001b[32m     16\u001b[39m weights = torch.softmax(weights,dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 4. åŠ æƒèšåˆ (Aggregate) = weights @ V\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# out = ...\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m out = weights @ \u001b[43mV\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# --- ä»£ç ç»“æŸ ---\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out, weights\n",
            "\u001b[31mNameError\u001b[39m: name 'V' is not defined"
          ]
        }
      ],
      "source": [
        "def raw_attention(q, k, v):\n",
        "    d_k = q.shape[-1] \n",
        "    \n",
        "    # --- TODO: è¯·åœ¨ä¸‹é¢è¡¥å…¨ä»£ç  ---\n",
        "    \n",
        "    # 1. è®¡ç®— Attention Scores (weights) = Q @ K^T\n",
        "    # weights shape åº”è¯¥æ˜¯ (B, T, T)\n",
        "    # weights = ...\n",
        "    weights = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    \n",
        "    # 2. ç¼©æ”¾ (Scale)\n",
        "    # weights = ...\n",
        "    \n",
        "    # 3. å½’ä¸€åŒ– (Softmax)\n",
        "    # weights = ...\n",
        "    weights = torch.softmax(weights,dim=-1)\n",
        "    # 4. åŠ æƒèšåˆ (Aggregate) = weights @ V\n",
        "    # out = ...\n",
        "    out = weights @ V\n",
        "    # --- ä»£ç ç»“æŸ ---\n",
        "    \n",
        "    return out, weights\n",
        "\n",
        "# è¿è¡Œæµ‹è¯•ï¼ˆå–æ¶ˆä¸‹é¢ä¸€è¡Œçš„æ³¨é‡Šæ¥æµ‹è¯•ï¼‰\n",
        "out, weights = raw_attention(query, key, value)\n",
        "print(\"Output shape:\", out.shape) # åº”è¯¥æ˜¯ (1, 8, 16)\n",
        "print(\"Weights shape:\", weights.shape) # åº”è¯¥æ˜¯ (1, 8, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬äºŒæ­¥ï¼šåŠ å…¥å› æœæ©ç  (Causal Masking)\n",
        "\n",
        "å¯¹äº GPT (Decoder-only) æ¨¡å‹ï¼Œ**Token ä¸èƒ½çœ‹åˆ°æœªæ¥çš„ä¿¡æ¯**ã€‚ä¾‹å¦‚ï¼Œç”Ÿæˆç¬¬ 5 ä¸ªå­—çš„æ—¶å€™ï¼Œåªèƒ½çœ‹ç¬¬ 1,2,3,4,5 ä¸ªå­—ï¼Œä¸èƒ½å·çœ‹ç¬¬ 6 ä¸ªå­—ã€‚\n",
        "\n",
        "æˆ‘ä»¬éœ€è¦åœ¨ Softmax ä¹‹å‰ï¼Œå°†çŸ©é˜µçš„ä¸Šä¸‰è§’éƒ¨åˆ†è®¾ç½®ä¸ºè´Ÿæ— ç©·å¤§ (`-inf`)ã€‚\n",
        "\n",
        "### ğŸ“ ç»ƒä¹  2: å®ç° Causal Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def causal_attention(q, k, v):\n",
        "    d_k = q.shape[-1]\n",
        "    T = q.shape[1] #..\n",
        "\n",
        "    \n",
        "    # 1. è®¡ç®— Q @ K^T å¹¶ç¼©æ”¾\n",
        "    weights = q @ k.transpose(-2, -1) * (d_k ** -0.5)\n",
        "    \n",
        "    # --- TODO: å®ç° Masking ---\n",
        "    # åˆ›å»ºä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µ (tril)ï¼Œå…¶ä¸­ 1 ä»£è¡¨å¯è§ï¼Œ0 ä»£è¡¨ä¸å¯è§\n",
        "    # ä½¿ç”¨ torch.tril å’Œ torch.ones\n",
        "    # mask = ...\n",
        "    \n",
        "    # ä½¿ç”¨ masked_fill å°† mask ä¸º 0 çš„ä½ç½®å¡«å……ä¸º float('-inf')\n",
        "    # weights = weights.masked_fill(...)\n",
        "    \n",
        "    # --- ä»£ç ç»“æŸ ---\n",
        "    \n",
        "    weights = F.softmax(weights, dim=-1)\n",
        "    out = weights @ v\n",
        "    return out, weights\n",
        "\n",
        "# è¿è¡Œæµ‹è¯•\n",
        "# out, wei = causal_attention(query, key, value)\n",
        "# print(\"Output shape:\", out.shape)\n",
        "# print(\"Weights sample (row 0):\", wei[0, 0, :]) # åº”è¯¥åªèƒ½çœ‹åˆ°ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œå…¶ä»–æ¥è¿‘ 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ç¬¬ä¸‰æ­¥ï¼šå°è£…æˆ nn.Module\n",
        "\n",
        "ç°åœ¨æˆ‘ä»¬å°†é€»è¾‘å°è£…åˆ°ä¸€ä¸ª PyTorch ç±»ä¸­ã€‚è¿™æ¬¡æˆ‘ä»¬ä¼šåŠ å…¥ `nn.Linear` æ¥ç”Ÿæˆ Q, K, Vã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SingleHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, head_size, block_size):\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        \n",
        "        # å®šä¹‰ä¸‰ä¸ªçº¿æ€§å±‚\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        \n",
        "        # æ³¨å†Œ mask ä¸ä½œä¸ºæ¨¡å‹å‚æ•°æ›´æ–°\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x çš„ç»´åº¦: (Batch, Time-step, Channels)\n",
        "        B, T, C = x.shape\n",
        "        \n",
        "        # --- TODO: å®Œæˆå‰å‘ä¼ æ’­ ---\n",
        "        # 1. ç”Ÿæˆ k, q, v (é€šè¿‡ self.key(x) ç­‰)\n",
        "        \n",
        "        # 2. è®¡ç®— attention scores (å¸¦ç¼©æ”¾)\n",
        "        \n",
        "        # 3. Apply mask (ä½¿ç”¨ self.tril[:T, :T])\n",
        "        \n",
        "        # 4. Softmax\n",
        "        \n",
        "        # 5. Aggregate output\n",
        "        \n",
        "        out = None # ä¿®æ”¹è¿™é‡Œ\n",
        "        \n",
        "        return out\n",
        "\n",
        "# æµ‹è¯• Module\n",
        "model = SingleHeadAttention(n_embd=32, head_size=16, block_size=8)\n",
        "x = torch.randn(1, 8, 32)\n",
        "out = model(x)\n",
        "print(\"Module Output shape:\", out.shape) # (1, 8, 16)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
